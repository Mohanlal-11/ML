{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df6325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c914427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from yolo.model import box_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b3175df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40, 40, 3, 85) float32\n",
      "(1, 20, 20, 3, 85) float32\n",
      "(1, 10, 10, 3, 85) float32\n",
      "torch.Size([1, 40, 40, 3, 85]) torch.float32\n",
      "torch.Size([1, 20, 20, 3, 85]) torch.float32\n",
      "torch.Size([1, 10, 10, 3, 85]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for i in range(3):\n",
    "    with open(f'yolov5_preds_{i}.pkl', 'rb') as fp:\n",
    "        predictions.append(pickle.load(fp))\n",
    "\n",
    "for pred in predictions:\n",
    "    print(pred.shape, pred.dtype)\n",
    "\n",
    "predictions_tensor = [torch.from_numpy(preds) for preds in predictions]\n",
    "for pred in predictions_tensor:\n",
    "    print(pred.shape, pred.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "675e407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "strides = (8,16,32)\n",
    "anchors = [\n",
    "            [[10, 13], [16, 30], [33, 23]],\n",
    "            [[30, 61], [62, 45], [59, 119]],\n",
    "            [[116, 90], [156, 198], [373, 326]]\n",
    "        ]\n",
    "detections = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecfcdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(preds, image_shapes, scale_factors, max_size, score_thresh, nms_thresh, merge=True):\n",
    "    anchors_tens = torch.tensor(anchors)\n",
    "    ids, ps, boxes = [], [], []\n",
    "    for pred, stride, wh in zip(preds, strides, anchors_tens): # 3.54s\n",
    "        pred = torch.sigmoid(pred)\n",
    "        n, y, x, a = torch.where(pred[..., 4] > score_thresh)\n",
    "        # print(f'{n}\\n, {y}\\n, {x}\\n, {a}\\n')\n",
    "        p = pred[n, y, x, a]\n",
    "        # print(f'selected preds: {p}')\n",
    "        xy = torch.stack((x, y), dim=1)\n",
    "        # print(f'selected xy: {xy}')\n",
    "        xy = (2 * p[:, :2] - 0.5 + xy) * stride\n",
    "        wh = 4 * p[:, 2:4] ** 2 * wh[a]\n",
    "        box = torch.cat((xy, wh), dim=1)\n",
    "        \n",
    "        ids.append(n)\n",
    "        ps.append(p)\n",
    "        boxes.append(box)\n",
    "        \n",
    "    ids = torch.cat(ids)\n",
    "    ps = torch.cat(ps)\n",
    "    boxes = torch.cat(boxes)\n",
    "    # print(\"selected boxes\",boxes)\n",
    "    boxes = box_ops.cxcywh2xyxy(boxes)\n",
    "    # print(boxes)\n",
    "    logits = ps[:, [4]] * ps[:, 5:]\n",
    "    indices, labels = torch.where(logits > score_thresh) # 4.94s\n",
    "    ids, boxes, scores = ids[indices], boxes[indices], logits[indices, labels]\n",
    "    \n",
    "    results = []\n",
    "    for i, im_s in enumerate(image_shapes): # 20.97s\n",
    "        keep = torch.where(ids == i)[0] # 3.11s\n",
    "        box, label, score = boxes[keep], labels[keep], scores[keep]\n",
    "        #ws, hs = boxes[:, 2] - boxes[:, 0], boxes[:, 3] - boxes[:, 1] # 0.27s\n",
    "        #keep = torch.where((ws >= self.min_size) & (hs >= self.min_size))[0] # 3.33s\n",
    "        #boxes, objectness, logits = boxes[keep], objectness[keep], logits[keep] # 0.36s\n",
    "        \n",
    "        if len(box) > 0:\n",
    "            box[:, 0].clamp_(0, im_s[1]) # 0.39s\n",
    "            box[:, 1].clamp_(0, im_s[0]) #~\n",
    "            box[:, 2].clamp_(0, im_s[1]) #~\n",
    "            box[:, 3].clamp_(0, im_s[0]) #~\n",
    "            \n",
    "            keep = box_ops.batched_nms(box, score, label, nms_thresh, max_size) # 4.43s\n",
    "            keep = keep[:detections]\n",
    "            \n",
    "            nms_box, nms_label = box[keep], label[keep]\n",
    "            if merge: # slightly increase AP, decrease speed ~14%\n",
    "                mask = nms_label[:, None] == label[None]\n",
    "                iou = (box_ops.box_iou(nms_box, box) * mask) > nms_thresh # 1.84s\n",
    "                weights = iou * score[None] # 0.14s\n",
    "                nms_box = torch.mm(weights, box) / weights.sum(1, keepdim=True) # 0.55s\n",
    "                \n",
    "            box, label, score = nms_box / scale_factors[i], nms_label, score[keep] # 0.30s\n",
    "        results.append(dict(boxes=box, labels=label, scores=score)) # boxes format: (xmin, ymin, xmax, ymax)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "356ca4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo.model.transform import Transformer\n",
    "import cv2\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1d69219",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "min_size=320, max_size=320, stride=max(strides))\n",
    "transformer.eval()\n",
    "ori_img = cv2.imread(\"test_one/000000317863.jpg\")\n",
    "resized_img = cv2.resize(ori_img, (320, 320))\n",
    "img = transforms.ToTensor()(ori_img)\n",
    "images, targets, scale_factors, image_shapes = transformer([img], targets=None)\n",
    "max_size = max(images.shape[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aba01158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[207.5607, 165.4065, 267.2700, 215.4290],\n",
      "        [194.9681, 180.0877, 264.3637, 214.4550],\n",
      "        [208.8387, 173.3563, 265.4771, 217.8499],\n",
      "        [207.2782, 172.2788, 266.4529, 219.1154],\n",
      "        [182.8741,  47.3400, 278.9813, 196.6384],\n",
      "        [183.1544,  48.9135, 278.5332, 195.0475],\n",
      "        [183.4637,  47.9795, 278.3604, 195.7554],\n",
      "        [183.7036,  48.6794, 279.6385, 197.2514],\n",
      "        [184.0784,  48.9719, 279.8970, 196.6795],\n",
      "        [184.4643,  48.1092, 279.6195, 197.4302],\n",
      "        [184.8304,  48.3282, 282.0228, 197.2558],\n",
      "        [184.6781,  49.6504, 282.3526, 195.9738],\n",
      "        [184.9131,  48.3072, 282.2008, 196.6118],\n",
      "        [182.8175,  57.2802, 281.7211, 197.1939],\n",
      "        [183.8640,  57.7651, 281.3806, 196.4873],\n",
      "        [184.1888,  56.7443, 281.2935, 197.4725],\n",
      "        [196.7213, 169.3411, 267.6656, 215.3331],\n",
      "        [196.9402, 169.3810, 267.5216, 215.4283],\n",
      "        [196.6767, 169.0664, 267.5294, 215.3776],\n",
      "        [187.4988, 179.6867, 268.2648, 218.8331],\n",
      "        [187.2930, 179.4818, 268.3215, 218.9551],\n",
      "        [187.3592, 178.2591, 268.3213, 219.4621],\n",
      "        [189.8097, 168.1273, 267.9826, 218.8866],\n",
      "        [190.0426, 167.6875, 267.6404, 219.1934],\n",
      "        [189.8482, 165.6548, 267.6863, 219.0383],\n",
      "        [185.0675,  46.5104, 280.9312, 199.0888],\n",
      "        [185.0512,  46.7984, 280.7820, 198.5023],\n",
      "        [185.2434,  46.5938, 281.1702, 198.2262],\n",
      "        [183.4698,  48.3243, 281.4240, 198.3567],\n",
      "        [183.5481,  48.7261, 281.3863, 198.1900],\n",
      "        [183.9611,  48.3556, 280.9588, 198.0053],\n",
      "        [183.9048,  48.4731, 281.1328, 198.9224],\n",
      "        [184.0795,  48.5075, 281.0687, 198.5541],\n",
      "        [184.6501,  47.8971, 280.6636, 199.0164],\n",
      "        [194.7481, 167.9355, 269.7632, 218.1058],\n",
      "        [193.6349, 166.3085, 270.4236, 221.0017],\n",
      "        [191.4671, 171.3707, 268.4097, 220.3345],\n",
      "        [193.4207, 166.3068, 267.8620, 217.5215],\n",
      "        [192.7572, 164.3147, 268.6623, 220.4805]])\n"
     ]
    }
   ],
   "source": [
    "res = inference(predictions_tensor, image_shapes, scale_factors, max_size, 0.3, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575561e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mohanlal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
